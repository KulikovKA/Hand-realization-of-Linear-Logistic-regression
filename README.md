# Hand-realization-of-Linear-Logistic-regression

# 1. Реализация Линейной Регрессии: Sklearn vs Градиентный Спуск

Этот ноутбук сравнивает библиотечную реализацию линейной регрессии с ручной реализацией через градиентный спуск на датасете Boston Housing.

### 1. Библиотечная реализация (sklearn)
Используется стандартный класс LinearRegression.
- Выводит коэффициенты и считает метрику **MSE** для сравнения с ручным методом.

### 2. Ручная реализация (Manual Implementation)
Алгоритм реализован с нуля для понимания математики процесса.


**Реализованные функции:**
- *predict(X, w)*: Вычисляет предсказание модели как взвешенную сумму признаков.
- *mse(y, y_pred)*: Функция потерь. Считает среднеквадратичную ошибку между реальными и предсказанными значениями.
- *gradient(X, w, y, y_pred)*: Вычисляет частные производные функции потерь по каждому весу для шага обновления.

**Цикл обучения:**
- Итеративное обновление весов по антиградиенту с шагом обучения *learning_rate=0.003*.
- Вывод текущей ошибки каждые 20 итераций для контроля сходимости.


---


# 2. Анализ Логистической Регрессии


### 1. Подготовка данных
Производится очистка и генерация признаков :
- **Новые признаки:** *FamilySize* (размер семьи) и *IsAlone* (одиночка).
- **Категоризация:** *Age* разбивается на группы (0: <18, 1: 18-55, 2: >55), *Sex* кодируется в бинарный формат.
- **Очистка:** Удаление пропусков и лишних столбцов (*Cabin*, *Ticket*, *Name* и др.).

### 2. Эксперименты с Scikit-Learn
Проводится обучение трех вариаций модели для демонстрации понятий ML:

1.  **Нормальная модель (*model_normal*):**
    - Использует регуляризацию *elasticnet*.
    - Метрики на Train/Test близки $\rightarrow$ хорошее обобщение.
2.  **Недообученная модель (*model_low*):**
    - Ограниченное число итераций (*max_iter=100*), отсутствие регуляризации.
    - Показатели метрик ниже оптимальных.
3.  **Переобученная модель (*model_ret*):**
    - Обучается на малом наборе данных (20% train).
    - Высокая точность на трейне, но низкая на тесте (Overfitting).

### 3. Ручная реализация (Logistic Regression from scratch)
Реализация логистической регрессии "с нуля" методом градиентного спуска.

**Ключевые этапы:**
- **Нормализация:** $Z$-score стандартизация входных данных (*X_norm*).
- **Сигмоида (*sigmoid*):** Функция активации для преобразования линейного выхода в вероятность:
  $$ \sigma(z) = \frac{1}{1 + e^{-z}} $$
- **Предсказание (*predict*):** Линейная комбинация весов и признаков.
- **Градиент (*gradient*):** Вычисление направления для обновления весов с целью минимизации ошибки.
- **Порог (*activation_prob*):** Перевод вероятностей в классы (0 или 1) с порогом 0.5.

### 4. Результаты
Ручная реализация показала конкурентоспособные результаты по сравнению с библиотечной версией *sklearn*:
- **Accuracy:** ~0.79 (Manual) vs ~0.76 (Sklearn)
- Алгоритм успешно сходится за 500 итераций.
